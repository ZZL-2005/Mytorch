<div align="center">

<img src="https://dummyimage.com/820x160/111/fff&text=MyTorch" alt="MyTorch Banner" />

<h1>MyTorch</h1>
<p><b>ä¸€ä¸ªåŸºäº <code>NumPy</code> ä»é›¶å®ç°çš„å¯é˜…è¯» / å¯å®éªŒ / å¯æ‰©å±•çš„æç®€æ·±åº¦å­¦ä¹  & è‡ªåŠ¨æ±‚å¯¼æ¡†æ¶</b></p>

<p>
<a href="#"> <img src="https://img.shields.io/badge/Python-3.9%2B-blue?logo=python" alt="Python"/> </a>
<a href="#license"> <img src="https://img.shields.io/badge/License-MIT-green" alt="License"/> </a>
<a href="#roadmap"> <img src="https://img.shields.io/badge/Status-Experimental-orange" alt="Status"/> </a>
<a href="#faq"> <img src="https://img.shields.io/badge/Docs-Progress-blueviolet" alt="Docs"/> </a>
<a href="#benchmarks"> <img src="https://img.shields.io/badge/Perf-Edu%20Priority-lightgrey" alt="Perf"/> </a>
<a href="#-å¼€å‘--è´¡çŒ®"> <img src="https://img.shields.io/badge/PRs-Welcome-brightgreen" alt="PRs"/> </a>
</p>

<p>
<b>æ•™å­¦ä¼˜å…ˆ Â· ç»“æ„æ¸…æ™° Â· ä»£ç æœ€å° Â· æ€è·¯é€æ˜</b>
</p>

<sub>English summary at the bottom Â· è‹±æ–‡ç®€ä»‹è§æ–‡æœ«</sub>

</div>

> â€œä¸è¦ç›´æ¥è·³è¿›å¤§å‹æ¡†æ¶é»‘ç®±â€”â€”å…ˆä»ä¸€ä¸ªä½ èƒ½å®Œæ•´è¯»å®Œçš„å®ç°å¼€å§‹ã€‚â€ MyTorch æ—¨åœ¨å¸®åŠ©åˆå­¦è€…ç†è§£ **å¼ é‡ â†’ è®¡ç®—å›¾ â†’ è‡ªåŠ¨æ±‚å¯¼ â†’ å‚æ•°æ›´æ–° â†’ è®­ç»ƒå¾ªç¯** çš„æœ€å°é—­ç¯ã€‚é¡¹ç›®åŒ…å« CIFARâ€‘10 å¤šç»„å®éªŒï¼ˆå­¦ä¹ ç‡ / BatchSize / ä¼˜åŒ–å™¨ / æ­£åˆ™ / Hidden Size ç­‰ï¼‰åŠæŠ¥å‘Šä¸å¯è§†åŒ–ã€‚æ•´ä½“é£æ ¼åšæŒ *Minimal Working Implementation*ï¼šå»é™¤å™ªå£°ï¼Œä»…ä¿ç•™æ ¸å¿ƒå†³ç­–ç‚¹ã€‚

---

## ğŸ”— å¿«é€Ÿå¯¼èˆª (Table of Contents)

- [âœ¨ ç‰¹æ€§æ¦‚è§ˆ](#-ç‰¹æ€§æ¦‚è§ˆ)
- [ğŸ“‚ ç›®å½•ç»“æ„](#-ç›®å½•ç»“æ„ç²¾ç®€è§†å›¾)
- [âš™ï¸ å®‰è£…](#ï¸-å®‰è£…)
- [ğŸš€ å¿«é€Ÿä¸Šæ‰‹](#-å¿«é€Ÿä¸Šæ‰‹)
- [ğŸ§© æ ¸å¿ƒè®¾è®¡](#-æ ¸å¿ƒè®¾è®¡è¯´æ˜)
- [ğŸ—ï¸ æ¶æ„ç¤ºæ„](#-æ¶æ„ç¤ºæ„)
- [ğŸ“ˆ å®éªŒä¸ç»“æœ](#-å®éªŒä¸ç»“æœæ¦‚è¦)
- [ğŸ§ª Benchmarks](#benchmarks)
- [ğŸ› ï¸ å¼€å‘ / è´¡çŒ®](#-å¼€å‘--è´¡çŒ®)
- [ğŸ—ºï¸ è·¯çº¿å›¾ / Roadmap](#-è·¯çº¿å›¾--todo)
- [â“ FAQ](#-faq)
- [ğŸ“œ è®¸å¯](#-è®¸å¯)
- [ğŸ™Œ è‡´è°¢](#-è‡´è°¢)
- [ğŸ‘¤ ä½œè€…](#-ä½œè€…)
- [English Brief](#english-brief)

---

## âœ¨ ç‰¹æ€§æ¦‚è§ˆ

- ğŸ”¢ å¼ é‡ç±» `MyTensor`ï¼šå°è£… `numpy.ndarray`ï¼Œè®°å½•çˆ¶èŠ‚ç‚¹ä¸ç”Ÿæˆç®—å­ï¼Œæ”¯æŒæ¢¯åº¦ç´¯ç§¯
- ğŸ”„ è‡ªåŠ¨æ±‚å¯¼ï¼šæ˜¾å¼æ„å»ºæœ‰å‘æ— ç¯è®¡ç®—å›¾ + æ‹“æ‰‘æ’åºåå‘ä¼ æ’­
- ğŸ§® å¸¸ç”¨ç®—å­ï¼šåŠ  / å‡ / ä¹˜ / çŸ©é˜µä¹˜ / Reshape / ReLU / Softmax / CrossEntropyLoss ç­‰
- ğŸ§  æ¨¡å‹æ¨¡å—åŒ–ï¼š`Module` / `Linear` / `MLP` / ï¼ˆå¯æ‰©å±• CNN å±‚ï¼‰
- ğŸ› ï¸ ä¼˜åŒ–å™¨ï¼šSGDã€Momentumã€Nesterovã€Adagradã€RMSPropã€Adamï¼ˆç»Ÿä¸€å‚æ•°æ›´æ–°æ¥å£ï¼‰
- ğŸ“¦ æ•°æ®å°è£…ï¼š`NumpyDataset` + `DataLoader`ï¼ˆæ‰¹ç”Ÿæˆã€æ‰“ä¹±ã€æ ‡å‡†åŒ–ï¼‰
- ğŸ§ª å®éªŒè„šæœ¬ï¼šå­¦ä¹ ç‡ / æ‰¹å¤§å° / ä¼˜åŒ–å™¨å¯¹æ¯” / Hidden Size / æƒé‡è¡°å‡
- ğŸ“ˆ æ—¥å¿—è®°å½•ï¼šCSV + JSON + æœ€ä¼˜æƒé‡æŒä¹…åŒ–ï¼ˆ`weights.npz` / `best_model.npz`ï¼‰
- ğŸ§¾ æŠ¥å‘Šäº§å‡ºï¼šLaTeX ç”Ÿæˆå®éªŒæŠ¥å‘Šï¼ˆ`report/main.pdf`ï¼‰

---

## ğŸ“‚ ç›®å½•ç»“æ„ï¼ˆç²¾ç®€è§†å›¾ï¼‰

```text
mytorch/
â”œâ”€ pyproject.toml        # æ‰“åŒ… & ä¾èµ–
â”œâ”€ train.py              # è®­ç»ƒè„šæœ¬ï¼ˆMLP on CIFAR-10ï¼‰
â”œâ”€ dataprocess.py        # æ•°æ®é›†è¯»å–ä¸åˆ’åˆ†
â”œâ”€ model/                # é¢å¤–æ¨¡å‹ï¼ˆå¦‚ MLPï¼‰
â”œâ”€ src/mytorch/          # æ ¸å¿ƒåº“æºç 
â”‚  â”œâ”€ __init__.py        # å¯¼å‡º MyTensor, DataLoader, NumpyDataset
â”‚  â”œâ”€ mytensor.py        # è®¡ç®—å›¾ä¸è‡ªåŠ¨æ±‚å¯¼æ ¸å¿ƒ
â”‚  â”œâ”€ nn/                # æ¨¡å—ã€å±‚ã€æ¿€æ´»ä¸æŸå¤±
â”‚  â”œâ”€ operation/         # åŸå­ç®—å­ / åå‘è§„åˆ™
â”‚  â””â”€ mydata.py          # æ•°æ®ç®¡é“
â”œâ”€ logs/                 # è®­ç»ƒæ—¥å¿— (CSV/JSON)
â”œâ”€ save/                 # ä¸åŒå®éªŒé…ç½®ä¿å­˜çš„æƒé‡/æ—¥å¿—
â””â”€ report/main.pdf       # å®éªŒåˆ†æä¸å¯è§†åŒ–
```

---

## âš™ï¸ å®‰è£…

æœ¬åœ°å¼€å‘ï¼ˆå»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ / condaï¼‰ï¼š

```bash
git clone https://github.com/yourname/mytorch.git
cd mytorch
pip install -e .
```

ä»…ä¾èµ–ï¼š`numpy`ã€‚å¯é€‰å¼€å‘ä¾èµ–ï¼ˆ`pytest`, `ruff`, `mypy`ï¼‰è§ `pyproject.toml`ã€‚

---

## ğŸš€ å¿«é€Ÿä¸Šæ‰‹

åˆ›å»ºä¸€ä¸ªæœ€å°çš„çº¿æ€§åˆ†ç±»å™¨å¹¶æ‰§è¡Œå‰å‘ä¸åå‘ï¼š

```python
import numpy as np
from mytorch import MyTensor
from mytorch.nn.func import relu, matmul

x = MyTensor(np.random.randn(4, 3), requires_grad=True)
w = MyTensor(np.random.randn(3, 2), requires_grad=True)
b = MyTensor(np.zeros((2,)), requires_grad=True)

logits = matmul(x, w) + b  # çº¿æ€§
act = relu(logits)         # æ¿€æ´»
loss = act.sum()           # æ ‡é‡æŸå¤±
loss.backward()            # è§¦å‘åå‘ä¼ æ’­

print(w.grad)              # æŸ¥çœ‹æ¢¯åº¦
```

ä½¿ç”¨å†…ç½® `MLP` è®­ç»ƒ CIFARâ€‘10ï¼ˆæ‘˜è‡ª `train.py`ï¼‰ï¼š

```python
from mytorch import DataLoader, NumpyDataset
from model.mlp import MLP
from dataprocess import get_data
from mytorch.nn import CrossEntropyLoss
from mytorch.nn.optim import SGD  # è‹¥ä½ çš„ optim æ¨¡å—è·¯å¾„ä¸åŒè¯·è°ƒæ•´

(X_train, y_train), (X_val, y_val), (X_test, y_test) = get_data(path='cifar-10-python.tar.gz')
train_ds = NumpyDataset(X_train, y_train, dtype=np.float32, normalize=True)
train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, to_tensor=True)

model = MLP(in_dim=3*32*32, hidden=512, out_dim=10)
criterion = CrossEntropyLoss()
optim = SGD(model.parameters(), lr=0.1)

for xb, yb in train_loader:
    # å‡è®¾ yb å·²æ˜¯ MyTensorï¼Œå†…éƒ¨ç”¨ one-hotï¼ˆç¤ºä¾‹ç•¥ï¼‰
    logits = model(xb)
    # è¿™é‡Œåº”å°† yb å¤„ç†æˆ one-hotï¼›è®­ç»ƒè„šæœ¬ä¸­å·²æœ‰å°è£…
    # ...
    # loss = criterion(logits, y_one_hot)
    # loss.backward(); optim.step(); model.zero_grad()
    break  # æ¼”ç¤ºä¸€æ¬¡ batch
```

å‘½ä»¤è¡Œè®­ç»ƒï¼š

```bash
python train.py --lr 0.1 --batch-size 128 --epochs 50 --optimizer sgd --hidden 512
```

---

## ğŸ§© æ ¸å¿ƒè®¾è®¡è¯´æ˜

| ç»„ä»¶ | è¯´æ˜ | å…³é”®ç‚¹ |
|------|------|--------|
| `MyTensor` | æ•°æ® + grad + è®¡ç®—å›¾å…ƒä¿¡æ¯ | `parents` / `op` / æƒ°æ€§æ„å›¾ |
| æ‹“æ‰‘æ’åº | åå‘ä¼ æ’­é¡ºåºæ§åˆ¶ | æ£€æµ‹ç¯ + ååº DFS |
| åŸå­ç®—å­ | `forward` + `backward` å°è£… | ä¿æŒæœ€å°å¯æ‰©å±•æ¥å£ |
| `Module` | å‚æ•°å®¹å™¨ä¸å±‚æŠ½è±¡ | é€’å½’å‚æ•°æ”¶é›† / `zero_grad()` |
| ä¼˜åŒ–å™¨ | é€å‚æ•°çŠ¶æ€æ›´æ–° | ç»Ÿä¸€ `step()` + `weight_decay` |
| æ•°æ®åŠ è½½ | ç®€åŒ–ç‰ˆ `DataLoader` | æ´—ç‰Œ / Batch ç”Ÿæˆ / è½¬å¼ é‡ |

---

## ğŸ—ï¸ æ¶æ„ç¤ºæ„

```text
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚           User Code          â”‚
       â”‚  (train.py / examples / â€¦)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ forward() è°ƒç”¨
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Modules     â”‚  (Linear / MLP / Future CNN ...)
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ åˆ›å»ºç®—å­èŠ‚ç‚¹ (Operation)
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  MyTensor     â”‚  (data, grad, parents, op)
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ ç»„ç»‡ DAG (è®¡ç®—å›¾)
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Topological   â”‚  (ååºéå† -> åå‘é¡ºåº)
          â”‚   Sort        â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ è§¦å‘ backward()
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Ops.backward â”‚  (å±€éƒ¨æ¢¯åº¦è§„åˆ™)
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ ä¼ é€’ / ç´¯åŠ  grad
          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Optimizers    â”‚  (SGD / Adam ... æ›´æ–°å‚æ•°)
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> è®¾è®¡ç›®æ ‡ï¼šç”¨æœ€å°‘å±‚æ¬¡ä¼ è¾¾â€œ**å¼ é‡å³èŠ‚ç‚¹ + è¿ç®—å³è¾¹ + backward å³æ²¿è¾¹ä¼ æ’­æ¢¯åº¦**â€ã€‚

---

## ğŸ“ˆ å®éªŒä¸ç»“æœæ¦‚è¦

å·²å®Œæˆï¼š

1. å­¦ä¹ ç‡ (0.001 / 0.01 / 0.1 / 1.0) Ã— BatchSize (16 / 64 / 128 / 256)
2. ä¼˜åŒ–å™¨å¯¹æ¯”ï¼šSGD / Momentum / Nesterov / RMSProp / Adam / Adagrad
3. éšè—å±‚å®½åº¦ï¼š8 â†’ 512 å¯¹æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆçš„å½±å“
4. æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰ä¸æ³›åŒ–
5. æœ€ä½³æ¨¡å‹ä¿å­˜ (Val Acc Max) ä¸æµ‹è¯•é›†è¯„ä¼°

è®­ç»ƒæ›²çº¿ä¸å¯è§†åŒ–ï¼ˆè¯¦è§ `report/images/`ï¼‰ï¼š

<p align="center">
  <img src="report/images/loss_grid.png" width="40%" />
  <img src="report/images/optimizer_val_acc.png" width="40%" />
</p>

> æ›´å…¨é¢å›¾è¡¨è¯·æ‰“å¼€ï¼š`report/main.pdf`

---

## ğŸ§ª Benchmarks

> æœ¬é¡¹ç›®å®šä½â€œæ•™å­¦æ¸…æ™°â€ä¼˜å…ˆï¼Œæ€§èƒ½éä¸»è¦ä¼˜åŒ–æ–¹å‘ï¼›ä»¥ä¸‹ç»“æœä»…å±•ç¤ºæ•°é‡çº§ä¾›å‚è€ƒã€‚

| ä»»åŠ¡ | æ¨¡å‹ | Epochs | æ—¶é—´ (CPU i5) | æœ€ç»ˆ Val Acc | å¤‡æ³¨ |
|------|------|--------|--------------|--------------|------|
| CIFARâ€‘10 | MLP (512 hidden) | 20 | ~X åˆ†é’Ÿ | ~Y% | å•çº¿ç¨‹ NumPy |

ï¼ˆæ³¨ï¼šä½ å¯åœ¨æœ¬åœ°è¿è¡Œåå¡«å†™ X / Yï¼›æˆ–å¢åŠ  CNN åå†æ‰©å……è¡¨æ ¼ã€‚ï¼‰

---

## ğŸ› ï¸ å¼€å‘ / è´¡çŒ®

è¿è¡Œé™æ€æ£€æŸ¥ä¸æµ‹è¯•ï¼ˆè‹¥å·²å®‰è£…å¯é€‰ä¾èµ–ï¼‰ï¼š

```bash
ruff check .
mypy src/mytorch
pytest -q
```

å»ºè®®è´¡çŒ®æµç¨‹ï¼š

1. Fork & æ–°å»ºåˆ†æ”¯ï¼š`feat/op-softmax` ç­‰
2. æ·»åŠ ç®—å­ï¼šå®ç° forward/backward å¹¶åœ¨ `__all__` å¯¼å‡º
3. è¡¥å……æœ€å°å•å…ƒæµ‹è¯•ç”¨ä¾‹
4. æ›´æ–°æ–‡æ¡£ä¸ç¤ºä¾‹
5. æäº¤ PRï¼ˆç®€è¿°åŠ¨æœº / æ€§èƒ½ / API å…¼å®¹æ€§ï¼‰

---

## ğŸ—ºï¸ è·¯çº¿å›¾ / TODO

- [ ] æ›´å®Œå–„çš„ CNN / å·ç§¯ä¸æ± åŒ–å±‚
- [ ] è‡ªåŠ¨å¹¿æ’­æ”¯æŒä¸æ›´å®Œæ•´çš„å¼ é‡æ“ä½œ
- [ ] GPU (CuPy) å¯é€‰åç«¯é€‚é…
- [ ] æ›´ä¸°å¯Œçš„æŸå¤±å‡½æ•°ï¼ˆL1 / SmoothL1 / Label Smoothingï¼‰
- [ ] æ¢¯åº¦è£å‰ªä¸å­¦ä¹ ç‡è°ƒåº¦å™¨
- [ ] ç®€å•çš„å¯è§†åŒ–ä»ªè¡¨ï¼ˆloss / acc å®æ—¶ï¼‰

æŸ¥çœ‹ `TODO.md` è·å–æœ€æ–°è®¡åˆ’ã€‚

---

## â“ FAQ

**Q: ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ PyTorchï¼Ÿ**  
æ•™å­¦æˆæœ¬ï¼šPyTorch åº•å±‚åŒ…å«å¤§é‡ C++ / Kernel / JIT / Dispatcher æœºåˆ¶ï¼Œä¸åˆ©äºåˆå­¦è€…å®Œæ•´è¿½æº¯ã€‚MyTorch è®©ä½ åœ¨ < 2k è¡Œä»£ç é‡Œèµ°é€šæ ¸å¿ƒé“¾è·¯ã€‚

**Q: æ”¯æŒåŠ¨æ€å›¾è¿˜æ˜¯é™æ€å›¾ï¼Ÿ**  
å±äºâ€œå³æ—¶æ„å›¾â€ï¼ˆeagerï¼‰æ–¹å¼ï¼šæ¯æ¬¡å‰å‘éƒ½ä¼šæ„å»ºå½“å‰è®¡ç®—å›¾ï¼Œå¹¶åœ¨ `backward()` åé‡Šæ”¾å¼•ç”¨ã€‚

**Q: èƒ½æ‰©å±•åˆ° GPU å—ï¼Ÿ**  
ç†è®ºä¸Šå¯ä»¥æŠŠå†…éƒ¨ `numpy` æŠ½è±¡æ¢æˆ `cupy` æˆ–å†™ä¸€ä¸ªåç«¯é€‚é…å±‚ï¼›å½“å‰ Roadmap ä¸­æœ‰åˆ—å‡ºã€‚

**Q: å¯ä»¥å•†ç”¨å—ï¼Ÿ**  
éµå¾ª MIT Licenseï¼Œå¯è‡ªç”±ä½¿ç”¨ï¼Œä½†è¯·æ³¨æ„å®ƒä¸æ˜¯ä¸ºç”Ÿäº§æ€§èƒ½ä¸ç¨³å®šæ€§è®¾è®¡ã€‚

**Q: å¦‚ä½•è°ƒè¯•æ¢¯åº¦æ˜¯å¦æ­£ç¡®ï¼Ÿ**  
å¯åšï¼šæ•°å€¼æ¢¯åº¦å¯¹æ¯”ï¼ˆfinite differenceï¼‰ï¼›åç»­å¯æ·»åŠ  `gradient_check.py` è„šæœ¬ã€‚

---

## ğŸ“œ è®¸å¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT Licenseï¼ˆè‹¥ `LICENSE` æœªæ›´æ–°ï¼Œè¯·è¡¥å……ä½œè€…ä¸å¹´ä»½ï¼‰ã€‚

---

## ğŸ™Œ è‡´è°¢

- ç»å…¸æ¡†æ¶è®¾è®¡å¯å‘ï¼šPyTorch / TinyGrad / MicroGrad
- æ•°æ®é›†ï¼šCIFARâ€‘10
- æŒ‡å¯¼è¯¾ç¨‹ / åŠ©æ•™è®¨è®º

---

## ğŸ‘¤ ä½œè€…

**å¼ å­è·¯ (Zilu Zhang)**  
Email: <zhangzilu@bupt.edu.cn>  
æ—¶é—´ï¼š2025-10  

æ¬¢è¿äº¤æµï¼šå®ç°ã€é‡æ„å»ºè®®ã€è¯¾ç¨‹è®¨è®ºæˆ–æ•™å­¦ç”¨é€”å¼•ç”¨ã€‚

---

## English Brief

MyTorch is a minimal educational deep learning framework built on top of NumPy. It implements a dynamic computation graph, reverseâ€‘mode autodiff, a small set of neural network modules, classic optimizers, and a light data pipeline. Experiments on CIFARâ€‘10 (optimizers, learning rates, hidden sizes, weight decay) are included, together with a LaTeX report. The focus is clarity over performance: ideal for students who want to read and extend a framework from scratch.

### Citation (Optional)

If you reference this project in teaching material / reports:

```text
@misc{mytorch2025,
  title  = {MyTorch: A Minimal NumPy-based Autograd and Neural Network Framework},
  author = {Zilu Zhang},
  year   = {2025},
  url    = {https://github.com/yourname/mytorch}
}
```

---

<div align="center">
<sub>â€œBuild your own tools to really understand the ones you use.â€</sub>
</div>

